{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whiteHatpro/coursera_algorithmic_toolbox/blob/master/NLP_Data_Extractor(3)_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Backend Clean code\n"
      ],
      "metadata": {
        "id": "4rn-ngB96ZbH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "IzKDekS76XGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget | grep -v 'already satisfied' #(added section to remove clutter of dependancies alredy installed)\n",
        "!pip install PyPDF2 | grep -v 'already satisfied'\n",
        "!pip install textract | grep -v 'already satisfied'\n",
        "!pip install PyMuPDF | grep -v 'already satisfied'"
      ],
      "metadata": {
        "id": "S4ppuXuXsrgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "kphyjx4_zVmu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import bs4\n",
        "import wget\n",
        "import fitz\n",
        "import numpy as np\n",
        "import PyPDF2\n",
        "import textract\n",
        "import re\n",
        "from PyPDF2 import PdfReader\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from gensim.utils import simple_preprocess\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "import urllib.request\n",
        "from urllib.request import urlopen\n",
        "from urllib.request import urlparse\n",
        "from ctypes.util import find_library\n",
        "import csv\n",
        "from google.colab import drive\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "kFHlbTS3zVmx"
      },
      "outputs": [],
      "source": [
        "#DATA EXTRACTION BLOCK:\n",
        "\n",
        "#extracting the title and page number\n",
        "def titel(file):\n",
        "    reader = PdfReader(file)\n",
        "    meta = reader.metadata\n",
        "    #print(len(reader.pages))\n",
        "    #print(meta.title)\n",
        "    return meta,len(reader.pages)\n",
        "#titel(file)\n",
        "\n",
        "#extract the text from the pdf\n",
        "def text(file):\n",
        "    pdfReader = PyPDF2.PdfReader(file)\n",
        "    num_pages = len(pdfReader.pages)\n",
        "\n",
        "    count = 0\n",
        "    text = \"\"\n",
        "\n",
        "    while count < num_pages:\n",
        "        #error pageObj = pdfReader.getPage(count)\n",
        "        pageObj = pdfReader.pages[count]\n",
        "        count +=1\n",
        "        text += pageObj.extract_text()\n",
        "\n",
        "    if text != \"\":\n",
        "        text = text\n",
        "\n",
        "    else:\n",
        "        text = textract.process('words.txt', method='tesseract', language='eng')\n",
        "\n",
        "\n",
        "    text = text.encode('ascii','ignore').lower()\n",
        "    text_decoded = text.decode()\n",
        "    return text_decoded\n",
        "\n",
        "#Pagewise Implimentation\n",
        "\n",
        "\n",
        "def extract_page(file,num=0):\n",
        "    pdf_file = fitz.open(file)\n",
        "    page_texts = []\n",
        "\n",
        "    for page_number in range(len(pdf_file)):\n",
        "        page = pdf_file.load_page(page_number)\n",
        "        page_text = page.get_text()\n",
        "\n",
        "        # Encode the page text as ASCII and convert to lowercase\n",
        "        page_text = page_text.encode('ascii', 'ignore').lower()\n",
        "        page_text_decoded = page_text.decode()\n",
        "        page_texts.append(page_text_decoded)\n",
        "\n",
        "    #page=int(input(\"Enter Page Number: \"))\n",
        "    filepage = \"\"\n",
        "\n",
        "    if page_texts:\n",
        "      filepage = page_texts[num]\n",
        "\n",
        "\n",
        "    return filepage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "0-P7O2vszVmy"
      },
      "outputs": [],
      "source": [
        "#DATA REFINEMENT BLOCK\n",
        "\n",
        "def cleanText(text):\n",
        "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
        "    text = text.lower()\n",
        "    text = text.replace(\"/\", \" \")\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    text = text.replace(\"''\", \"\")\n",
        "    text = text.replace(\"  \", \" \")\n",
        "    text = text.replace(\"  \", \" \")\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    text = re.sub('[‘’“”…]', '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    return text\n",
        "round1 = lambda x: cleanText(x)\n",
        "\n",
        "#Remove stopwords\n",
        "stopwords_en = stopwords.words('english')\n",
        "additionalStopWords = [\"pg\",\"figure\",\"u\"]    #Remove the words that are present in the title\n",
        "stopwords_en.extend(additionalStopWords)\n",
        "def toggleStopword(newStopword):\n",
        "  if newStopword in stopwords_en:\n",
        "        stopwords_en.remove(newStopword)\n",
        "  else:\n",
        "        stopwords_en.append(newStopword)\n",
        "\n",
        "def removeStopwords(texts):\n",
        "    extract = ''\n",
        "    for word in simple_preprocess(texts):\n",
        "        if word not in stopwords_en:\n",
        "            extract = extract + ' ' + word\n",
        "    return extract\n",
        "\n",
        "#Lemmatize keywords\n",
        "def lemmatizeKeywords(keywords):\n",
        "    keywords_list = keywords.split(' ')\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    keywords_lemmatized = []\n",
        "    for keyword in keywords_list:\n",
        "        keywords_lemmatized.append(lemmatizer.lemmatize(keyword))\n",
        "    return ' '.join(keywords_lemmatized)\n",
        "\n",
        "\n",
        "def identifyFrequentKeywords(keywordsFlat,additionalWords=[\"\"]): #Additional words should be a list of strings.\n",
        "    keywords = keywordsFlat.split(' ')\n",
        "    while '' in keywords:\n",
        "        keywords.remove('')\n",
        "    keywordsFlat = ' '.join(keywords)\n",
        "    keywordCount = len(keywords)\n",
        "\n",
        "    trigrams_keys = []\n",
        "    trigrams_values = []\n",
        "    trigrams = nltk.trigrams(keywords)\n",
        "    frequency = nltk.FreqDist(trigrams)\n",
        "    for key,value in frequency.items():\n",
        "        if value > 1: #value->absolute count\n",
        "            trigrams_keys.append(' '.join(key))\n",
        "            trigrams_values.append(value/keywordCount) #->persisting relative frequency\n",
        "\n",
        "    bigrams_keys = []\n",
        "    bigrams_values = []\n",
        "    bigrams = nltk.bigrams(keywordsFlat.split(' '))\n",
        "    frequency = nltk.FreqDist(bigrams)\n",
        "    for key,value in frequency.items():\n",
        "        if value > 1:\n",
        "            bigrams_keys.append(' '.join(key))\n",
        "            bigrams_values.append(value/keywordCount)\n",
        "\n",
        "    monograms_keys = []\n",
        "    monograms_values = []\n",
        "    frequency = nltk.FreqDist(keywordsFlat.split(' '))\n",
        "    for key,value in frequency.items():\n",
        "        if value > 1:\n",
        "            monograms_keys.append(key)\n",
        "            monograms_values.append(value/keywordCount)\n",
        "\n",
        "    additionalWords_words=[]\n",
        "    additionalWords_values=[]\n",
        "    additionalWords=[word.lower() for word in additionalWords]\n",
        "    frequency = nltk.FreqDist(keywordsFlat.split(' '))\n",
        "    for key,value in frequency.items():\n",
        "      if key in additionalWords:\n",
        "        if value> 1:\n",
        "          #print(key,value)\n",
        "          additionalWords_words.append(key)\n",
        "          additionalWords_values.append(value/keywordCount)\n",
        "      # for\n",
        "      #   additionalWords_values.append([\"Not Present\"])\n",
        "        #print(additionalWords_values)\n",
        "\n",
        "\n",
        "    keys_selected = trigrams_keys + bigrams_keys + monograms_keys\n",
        "    keys_selected_values = trigrams_values + bigrams_values + monograms_values\n",
        "    keys_selected = [x for _, x in sorted(zip(keys_selected_values, keys_selected), reverse=True)]\n",
        "    keys_selected_values.sort(reverse=True)\n",
        "\n",
        "    #select top-n keywords\n",
        "    numberOfTopWords = 50\n",
        "    if len(keys_selected) > numberOfTopWords:\n",
        "        keys_selected        = keys_selected[:numberOfTopWords]  + additionalWords_words\n",
        "        keys_selected_values = keys_selected_values[:numberOfTopWords] + additionalWords_values\n",
        "\n",
        "    return keys_selected, keys_selected_values, keywordCount\n",
        "\n",
        "additionalKeywords=[\"\"]\n",
        "\n",
        "def toggleKeyword(newKeyword):\n",
        "  if newKeyword in additionalKeywords:\n",
        "        additionalKeywords.remove(newKeyword)\n",
        "  else:\n",
        "        additionalKeywords.append(newKeyword)\n",
        "\n",
        "def keywordExtractor(extractedPage):\n",
        "      x=extractedPage\n",
        "      x=cleanText(x)\n",
        "      x=lemmatizeKeywords(x)                                  #Turn words into their root ( Happily to Happy, faster to fast, etc)\n",
        "      x=removeStopwords(x)                                    #Only to be used if there's any specific words we want to remove\n",
        "      m=identifyFrequentKeywords(x,additionalKeywords)        #Extracts the CSV style entry in format [Word,Freq,PageWordCount] of our page's frequent keywords, we can add additional keywords to check as well if we wish\n",
        "      return m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "fdcp_ysIzVm1"
      },
      "outputs": [],
      "source": [
        "#WORD BASED FUNCTIONS\n",
        "\n",
        "def word_count(word, text1):                                      #Count of a specific word in the document\n",
        "    with open('/content/drive/MyDrive/test6.txt','w+') as f:\n",
        "        #f.write(cleanText(text(file1)))\n",
        "        f.write(text1)\n",
        "    with open('/content/drive/MyDrive/test6.txt') as f:\n",
        "        return ''.join(f).count(word)\n",
        "\n",
        "def word_cooccurrence(word1,word2,file):                        #Extracts list of pages where words 1 and 2 appear\n",
        "  page_list=[]\n",
        "  pages=titel(file)[1]\n",
        "  for i in range(pages):\n",
        "    n=extract_page(file,i)\n",
        "    n=n.lower()\n",
        "    if word1.lower() in n:\n",
        "      if word2.lower() in n:\n",
        "        page_list.append(i)\n",
        "  return page_list                                            #REMEMBER, this is the list of pages from index 0\n",
        "\n",
        "def wordTotal(pageList,file):                               #Extracts total number of words on the pages we are working on\n",
        "  wordTotal=0\n",
        "  for i in pageList:\n",
        "      n=extract_page(file,i)\n",
        "      m=keywordExtractor(n)\n",
        "      a,b,c=m\n",
        "      wordTotal+=c\n",
        "  return wordTotal\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#FREQUENCY CSV FUNCTIONS\n",
        "def CSVFreq(file):                                           #Get the total CSV of frequent keywords and their frequencies for our file\n",
        "  pages=titel(file)[1]\n",
        "  CSV=[]\n",
        "  for i in range(pages):\n",
        "      n=extract_page(file,i)\n",
        "      m=keywordExtractor(n)\n",
        "      a,b,c=m\n",
        "      print(i+1)\n",
        "      print(m)\n",
        "      for j in range(len(a)):\n",
        "          CSV.append([i+1,a[j],b[j],c])                    #Outputs CSV format [Page,Word,Freq,PageWordCount]\n",
        "  return CSV\n",
        "\n",
        "def associatedCSVFrequency(pageList, file):                 #Get the CSV of frequent keywords and their frequencies for a specific list of pages\n",
        "  CSV=[]\n",
        "  for i in pageList:\n",
        "      n=extract_page(file,i)\n",
        "      m=keywordExtractor(n)\n",
        "      a,b,c=m\n",
        "      #print(i)\n",
        "      #print(m)\n",
        "      for j in range(len(a)):\n",
        "          CSV.append([i+1,a[j],b[j],c])\n",
        "  return CSV\n",
        "\n",
        "def CSVAggregator(CSV):                                    #Reduce all duplicate words into a combined output CSV of weighted frequency\n",
        "  i=0\n",
        "  while(i<len(CSV)-1):\n",
        "    j=i+1\n",
        "    while(j<len(CSV)):\n",
        "        if CSV[i][1]==CSV[j][1]:\n",
        "          aggFreq=(CSV[i][2]*CSV[i][3] + CSV[j][2]*CSV[j][3])/(CSV[i][3]+CSV[j][3])\n",
        "          CSV[i]=[CSV[i][0],CSV[i][1],aggFreq,CSV[i][3]+CSV[j][3]]\n",
        "          CSV.pop(j)\n",
        "        j+=1\n",
        "    i+=1\n",
        "\n",
        "  return CSV\n",
        "\n",
        "def CSVNormalizer(CSV,wordTotal):                         #Instead of weighing the words by total occurance on the pages they appear on, we can weight their frequencies by a specific count\n",
        "    for i in CSV:\n",
        "      i[2]=i[2]*i[3]/wordTotal\n",
        "      i[3]=wordTotal\n",
        "    return CSV\n",
        "\n",
        "def SpecificCSV(file,word1,word2):                                  #leave either as a blank string if you dont want any extra check word\n",
        "  toggleKeyword(word1)                                              #Add our specific words to the keywords we are looking for in the file, regardless of if they are frequently occuring\n",
        "  toggleKeyword(word2)                                              #NEED TO DEBUG\n",
        "  x=word_cooccurrence(word1,word2,file)                             #List of Pages where our requested words are present\n",
        "  print(\"The pages that our words of interest \"+ word1+\" and \"+ word2 +\" appear on are:\")\n",
        "  for i in x:\n",
        "    print(i+1,end=\",\")\n",
        "  w=wordTotal(x,file)                                               #Total words on the pages where our requested words are present\n",
        "  y=associatedCSVFrequency(x, file)                                 #Produce CSV of the frequent words on our requested pages\n",
        "  z=CSVAggregator(y)                                                #Bunch together words that reoccur\n",
        "  CSVfromSpecificPagesWithWordsOfInterest=CSVNormalizer(z,w)        #Get the frequencies for the total number of words in our concerned pages\n",
        "  toggleKeyword(word1)                                              #Remove our added keyword to prevent clutter\n",
        "  toggleKeyword(word2)\n",
        "  return CSVfromSpecificPagesWithWordsOfInterest\n",
        "\n",
        "def SortCSV(CSV):                     #Sorts CSVs of our format by their frequency\n",
        "  sortedCSV=[]\n",
        "  while(CSV):\n",
        "    ifreq=[0,0,0,0]\n",
        "    for i in CSV:\n",
        "      if i[2]>ifreq[2]:\n",
        "        ifreq=i\n",
        "    sortedCSV.append(ifreq)\n",
        "    CSV.remove(ifreq)\n",
        "  return sortedCSV"
      ],
      "metadata": {
        "id": "ogTf0NxCRLbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "QkEF6mr-zVmx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20ccd079-1a7c-43f7-e6d6-a06010d58578"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Germany\n"
          ]
        }
      ],
      "source": [
        "#COUNTRY SETTER\n",
        "\n",
        "def SetCountry(country):#Write Country name, dont forget capitals first letter\n",
        "  drive.mount('/content/drive')\n",
        "  file = open('/content/drive/MyDrive/Sparsh_Policies/'+country+'.pdf','rb')\n",
        "  return file,country;\n",
        "\n",
        "file,country=SetCountry(\"Germany\")\n",
        "print(country)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word1=\"\"     #Try success\n",
        "word2=\"\"\n",
        "x=SpecificCSV(file,word1,word2)\n",
        "SortCSV(x)\n"
      ],
      "metadata": {
        "id": "zZ2fuZtEet5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#NEW FUNCTIONS DEMOBLOCK\n",
        "word1=\"\"\n",
        "word2=\"\"\n",
        "x=word_cooccurrence(word1,word2,file)  #Only use if you want to check the specific pages where 2 given words are coocccuring. Dont worry about capitals. If you want to check a single word/phrase, make the second word just \"\"\n",
        "toggleKeyword(word1)                   #Including keywords forcefully in selection\n",
        "toggleKeyword(word2)\n",
        "print(\"The pages that our words of interest \"+ word1+\" and \"+ word2 +\" appear on are:\")\n",
        "for i in x:\n",
        "  print(i+1,end=\",\")\n",
        "print(\"\\n---------------------\")\n",
        "print(\"The total number of words in our pages of interest are:\\n\")\n",
        "w=wordTotal(x,file)\n",
        "print(w)\n",
        "# x1=word_cooccurrence(\"\",\"\",file)       #Sample to get total umber of words in whole doc\n",
        "# w2=wordTotal(x1,file)\n",
        "# print(w2)\n",
        "print(\"---------------------\")\n",
        "print(\"Frequent words in these pages are:\\n\")\n",
        "y=associatedCSVFrequency(x, file)\n",
        "for i in y:\n",
        "  print(i)\n",
        "print(\"---------------------\")\n",
        "print(\"Aggregating and weighing duplicate words:\\n\")\n",
        "z=CSVAggregator(y)\n",
        "for i in z:\n",
        "  print(i)\n",
        "print(\"---------------------\")\n",
        "print(\"After normalizing frequency to the total number of words on these pages:\\n\")\n",
        "\n",
        "f=CSVNormalizer(z,w)                    # Normalize in accordance to number of words in pages where our requested words appear\n",
        "# f=CSVNormalizer(z,w2)                 # Normalize in accordance to number of words in whole doc\n",
        "\n",
        "toggleKeyword(word1)                    #Removing added keywords for cleanup\n",
        "toggleKeyword(word2)\n",
        "\n",
        "for i in f:\n",
        "  print(i)\n",
        "print(\"---------------------\")\n",
        "print(\"Sorting all of these words by frequency:\\n\")\n",
        "SortCSV(f)\n",
        "\n"
      ],
      "metadata": {
        "id": "BHep8eSjMcrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Removal of words in title\n",
        "    # (DONE) word- > list of pages where word is keyword - > relatiove frequency in all pages of words in which word is a keyword. Wieghted average or overall sum and averagess.\n",
        "x=CSVFreq(file)\n",
        "with open('/content/drive/MyDrive/Policy/'+country+'_Frequency_EXPERIMENTAL.csv','w+',newline='') as f:\n",
        "     writer = csv.writer(f)\n",
        "     writer.writerows(x)\n",
        "\n",
        "\n",
        "toggleStopword(country)      #IF YOU ADD A NEW STOPWORD HERE, ADD IT AT THE BOTTOM AS WELL SO THE ARRAY SELF CORRECTS\n",
        "toggleStopword('hydrogen')   #Add Hydrogen to stopwords\n",
        "\n",
        "\n",
        "x=CSVFreq(file)\n",
        "with open('/content/drive/MyDrive/Policy/'+country+'_Frequency(wo-Stopwords).csv','w+',newline='') as f:\n",
        "     writer = csv.writer(f)\n",
        "     writer.writerows(x)\n",
        "\n",
        "toggleStopword('hydrogen')  # Remove Hydrogen from stopwords again for next file\n",
        "toggleStopword(country)\n"
      ],
      "metadata": {
        "id": "EOXlToleSzxn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}